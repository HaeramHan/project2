set.seed(322)
k=10
data<-sample_frac(VA)
folds <- rep(1:k, length.out=nrow(data))
diags<-NULL
i=1
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$treat
as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}
summarize_all(diags,mean)
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)
#your code here
prob_knn <- predict(knn_fit,newdata = VA)
class_diag(prob_knn[,2], truth=VA$treat, positive="standard")
View(VA)
View(VA)
View(VA)
yes
View(VA)
n
no
no
View(VA)
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
library(MASS)
VA <- data.frame(VA)
# if your dataset needs tidying, do so here
VA$treat <- ifelse(VA$treat == 1,"standard","test")
head(VA)
# any other code here
?VA
VA %>% group_by(status) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(cell) %>% count()
VA %>% group_by(prior) %>% count()
library(cluster)
# clustering code here
VA_dat <- VA %>% dplyr::select(stime,age, Karn)
sil_width<- vector()
for (i in 2:10){
pam_fit <- pam(VA_dat, k=i)
sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_point(aes(x=1:10,y=sil_width))+geom_path(aes(x=1:10,y=sil_width))+xlab("clusters")+scale_x_continuous(name="k", breaks=1:10)
VA_pam <- VA_dat %>% pam(k=2)
plot(VA_pam, which= 2)
VA_pam$medoids
VA %>% slice(VA_pam$id.med)
library(GGally)
VA <- VA %>% mutate(cluster=as.factor(VA_pam$clustering))
ggpairs(VA, cols= 1:6, aes(color=cluster))
# PCA code here
princomp(VA_dat, cor=T) -> pca1
VA_dat
summary(pca1)
pcavar <- pca1$sdev^2
varprop = round(pcavar/sum(pcavar), 2)
ggplot() + geom_bar(aes(y = varprop, x = 1:3), stat = "identity") +
xlab("") + geom_path(aes(y = varprop, x = 1:3)) +
geom_text(aes(x = 1:3, y = varprop, label = round(varprop,
2)), vjust = 1, col = "white", size = 5) +
scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) +
scale_x_continuous(breaks = 1:10)
round(cumsum(pcavar)/sum(pcavar), 2)
round(cumsum(pcavar)/sum(pcavar), 3)
summary(pca1, loadings = T)
pca1mat <- pca1$scores %>% cor %>% round(10)
library(dplyr)
VA_dat %>% as.data.frame %>% dplyr::select(age) %>% mutate(PC1=pca1$scores[,1],
PC2= pca1$scores[,2])%>%
ggplot(aes(PC1,PC2, color = age)) +geom_point()
# linear classifier code here
logistic_fit <- glm(treat== "standard" ~ stime + age + Karn + diag.time, data=VA, family="binomial")
logistic_fit
prob_reg <- predict(logistic_fit, type="response")
class_diag(prob_reg, truth=VA$treat, positive ="standard")
y <- VA$treat
y<- factor(y, levels=c("standard","test"))
y_hat <- sample(c("standard","test"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("standard","test"))
table(actual = y, predicted = y_hat) %>% addmargins()
table(actual = y, predicted = y_hat) %>% addmargins()
summarize_all(diags,mean)
# cross-validation of linear classifier here
set.seed(322)
k=10
data<-sample_frac(VA)
folds <- rep(1:k, length.out=nrow(data))
diags<-NULL
i=1
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$treat
as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}
summarize_all(diags,mean)
summarize_all(diags,mean)
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)
#your code here
prob_knn <- predict(knn_fit,newdata = VA)
class_diag(prob_knn[,2], truth=VA$treat, positive="standard")
knn_fit2 <-knn3(factor(treat=="standard",levels=c("TRUE","FALSE")) ~ stime + age + Karn + diag.time, data=VA, k=5)
y_hat_knn <- predict(knn_fit2,VA)
table(truth= factor(VA$treat== "standard", levels=c("TRUE","FALSE")),
prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
np <- import("numpy", convert = FALSE)
library(reticulate)
py_run_string("x = 10")
np <- import("numpy", convert = FALSE)
py$age
pyton3
python3
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
rmean_age <- py$mean_age
use_python("/usr/local/bin/python")
py_run_string("x = 10")
np <- import("numpy", convert = FALSE)
use_python("/usr/bin/python3", required = F)
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
rmean_age <- py$mean_age
py$age
py$Age_mean
py$Age_mean
py$Age_mean
os <- import("os")
use_python("/usr/bin/python3", required = F)
py$Age_mean
py$Age_mean
py$Age_mean
py$Age_mean
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
library(MASS)
VA <- data.frame(VA)
# if your dataset needs tidying, do so here
VA$treat <- ifelse(VA$treat == 1,"standard","test")
head(VA)
# any other code here
?VA
VA %>% group_by(status) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(cell) %>% count()
VA %>% group_by(prior) %>% count()
library(cluster)
# clustering code here
VA_dat <- VA %>% dplyr::select(stime,age, Karn)
sil_width<- vector()
for (i in 2:10){
pam_fit <- pam(VA_dat, k=i)
sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_point(aes(x=1:10,y=sil_width))+geom_path(aes(x=1:10,y=sil_width))+xlab("clusters")+scale_x_continuous(name="k", breaks=1:10)
VA_pam <- VA_dat %>% pam(k=2)
plot(VA_pam, which= 2)
VA_pam$medoids
VA %>% slice(VA_pam$id.med)
library(GGally)
VA <- VA %>% mutate(cluster=as.factor(VA_pam$clustering))
ggpairs(VA, cols= 1:6, aes(color=cluster))
# PCA code here
princomp(VA_dat, cor=T) -> pca1
VA_dat
summary(pca1)
pcavar <- pca1$sdev^2
varprop = round(pcavar/sum(pcavar), 2)
ggplot() + geom_bar(aes(y = varprop, x = 1:3), stat = "identity") +
xlab("") + geom_path(aes(y = varprop, x = 1:3)) +
geom_text(aes(x = 1:3, y = varprop, label = round(varprop,
2)), vjust = 1, col = "white", size = 5) +
scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) +
scale_x_continuous(breaks = 1:10)
round(cumsum(pcavar)/sum(pcavar), 2)
round(cumsum(pcavar)/sum(pcavar), 3)
summary(pca1, loadings = T)
pca1mat <- pca1$scores %>% cor %>% round(10)
library(dplyr)
VA_dat %>% as.data.frame %>% dplyr::select(age) %>% mutate(PC1=pca1$scores[,1],
PC2= pca1$scores[,2])%>%
ggplot(aes(PC1,PC2, color = age)) +geom_point()
# linear classifier code here
logistic_fit <- glm(treat== "standard" ~ stime + age + Karn + diag.time, data=VA, family="binomial")
logistic_fit
prob_reg <- predict(logistic_fit, type="response")
class_diag(prob_reg, truth=VA$treat, positive ="standard")
y <- VA$treat
y<- factor(y, levels=c("standard","test"))
y_hat <- sample(c("standard","test"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("standard","test"))
table(actual = y, predicted = y_hat) %>% addmargins()
# cross-validation of linear classifier here
set.seed(322)
k=10
data<-sample_frac(VA)
folds <- rep(1:k, length.out=nrow(data))
diags<-NULL
i=1
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$treat
as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}
summarize_all(diags,mean)
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)
#your code here
prob_knn <- predict(knn_fit,newdata = VA)
class_diag(prob_knn[,2], truth=VA$treat, positive="standard")
py$Age_mean
library(reticulate)
py$Age_mean
library(reticulate)
os <- import("os")
use_python("/usr/bin/python3", required = F)
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
py$Age_mean
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))
class_diag <- function(score, truth, positive, cutoff=.5){
pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
tab<-table(truth, pred)
acc=sum(diag(tab))/sum(tab)
sens=tab[1,1]/rowSums(tab)[1]
spec=tab[2,2]/rowSums(tab)[2]
ppv=tab[1,1]/colSums(tab)[1]
#CALCULATE F1
f1=2*(sens*ppv)/(sens+ppv)
#CALCULATE EXACT AUC
truth<-as.numeric(truth=="TRUE")
ord<-order(score, decreasing=TRUE)
score <- score[ord]; truth <- truth[ord]
TPR=cumsum(truth)/max(1,sum(truth))
FPR=cumsum(!truth)/max(1,sum(!truth))
dup<-c(score[-1]>=score[-length(score)], FALSE)
TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
n <- length(TPR)
auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
library(MASS)
VA <- data.frame(VA)
# if your dataset needs tidying, do so here
VA$treat <- ifelse(VA$treat == 1,"standard","test")
head(VA)
# any other code here
?VA
# any other code here
?VA
VA %>% group_by(status) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(cell) %>% count()
VA %>% group_by(prior) %>% count()
library(cluster)
# clustering code here
VA_dat <- VA %>% dplyr::select(stime,age, Karn)
sil_width<- vector()
for (i in 2:10){
pam_fit <- pam(VA_dat, k=i)
sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_point(aes(x=1:10,y=sil_width))+geom_path(aes(x=1:10,y=sil_width))+xlab("clusters")+scale_x_continuous(name="k", breaks=1:10)
VA_pam <- VA_dat %>% pam(k=2)
plot(VA_pam, which= 2)
VA_pam$medoids
VA %>% slice(VA_pam$id.med)
library(GGally)
VA <- VA %>% mutate(cluster=as.factor(VA_pam$clustering))
ggpairs(VA, cols= 1:6, aes(color=cluster))
# PCA code here
princomp(VA_dat, cor=T) -> pca1
VA_dat
summary(pca1)
pcavar <- pca1$sdev^2
varprop = round(pcavar/sum(pcavar), 2)
ggplot() + geom_bar(aes(y = varprop, x = 1:3), stat = "identity") +
xlab("") + geom_path(aes(y = varprop, x = 1:3)) +
geom_text(aes(x = 1:3, y = varprop, label = round(varprop,
2)), vjust = 1, col = "white", size = 5) +
scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) +
scale_x_continuous(breaks = 1:10)
round(cumsum(pcavar)/sum(pcavar), 2)
round(cumsum(pcavar)/sum(pcavar), 3)
summary(pca1, loadings = T)
pca1mat <- pca1$scores %>% cor %>% round(10)
library(dplyr)
VA_dat %>% as.data.frame %>% dplyr::select(age) %>% mutate(PC1=pca1$scores[,1],
PC2= pca1$scores[,2])%>%
ggplot(aes(PC1,PC2, color = age)) +geom_point()
# linear classifier code here
logistic_fit <- glm(treat== "standard" ~ stime + age + Karn + diag.time, data=VA, family="binomial")
logistic_fit
prob_reg <- predict(logistic_fit, type="response")
class_diag(prob_reg, truth=VA$treat, positive ="standard")
y <- VA$treat
y<- factor(y, levels=c("standard","test"))
y_hat <- sample(c("standard","test"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("standard","test"))
table(actual = y, predicted = y_hat) %>% addmargins()
# cross-validation of linear classifier here
set.seed(322)
k=10
data<-sample_frac(VA)
folds <- rep(1:k, length.out=nrow(data))
diags<-NULL
i=1
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$treat
as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}
summarize_all(diags,mean)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)
#your code here
prob_knn <- predict(knn_fit,newdata = VA)
class_diag(prob_knn[,2], truth=VA$treat, positive="standard")
knn_fit2 <-knn3(factor(treat=="standard",levels=c("TRUE","FALSE")) ~ stime + age + Karn + diag.time, data=VA, k=5)
y_hat_knn <- predict(knn_fit2,VA)
table(truth= factor(VA$treat== "standard", levels=c("TRUE","FALSE")),
prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))
knn_fit2 <-knn3(factor(treat=="standard",levels=c("TRUE","FALSE")) ~ stime + age + Karn + diag.time, data=VA, k=5)
y_hat_knn <- predict(knn_fit2,VA)
table(truth= factor(VA$treat== "standard", levels=c("TRUE","FALSE")),
prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))
# cross-validation of np classifier here
set.seed(322)
k=10
data<-sample_frac(VA)
folds <- rep(1:k, length.out=nrow(data))
diags<-NULL
i=1
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
truth<-test$treat
fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)
probs <- predict(fit, newdata=test)[,2]
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}
summarize_all(diags,mean)
Discussion
For the non parametric classifier the AUC without cross validation shows has a value of 0.728 meaning this model is pretty fair. However, after cross validation the AUC had a value of 0.705. This gap is somewhat small meaning there is some underfitting happening. Overall this model is better at predicting new observations than the linear classification. This model had an AUC value of 0.70 meaning that this model is fair.
### Regression/Numeric Prediction
# regression model code here
fit<-lm(age~.,data=VA)
yhat_reg <-predict(fit)
yhat_reg
mean((VA$age-yhat_reg)^2)
# cross-validation of regression model here
set.seed(1234)
k=5
data<-VA[sample(nrow(VA)),]
folds<-cut(seq(1:nrow(VA)),breaks=k,labels=F)
diags<-NULL
for(i in 1:k){
train<-data[folds!=i,]
test<-data[folds==i,]
fit<-lm(age~.,data=train)
yhat<-predict(fit,newdata=test)
diags<-mean((test$age-yhat)^2)
}
mean(diags)
library(reticulate)
os <- import("os")
use_python("/usr/bin/python3", required = F)
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
py$Age_mean
np <- import("numpy", convert = FALSE)
mean(VA$age)
use_python("/usr/bin/python3")
use_python("/usr/bin/python")
use_python("/usr/bin/python3", required = F)
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
os <- import("os")
py$Age_mean
py$Age_mean
py$Age_mean
py_run_string("x = 10"); py$Age_mean
py$Age_mean
py$Age_mean
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
py_run_string("x = 10"); py$Age_mean
py$Age_mean
py_run_string("x = 10")
py$Age_mean
packageVersion("rmarkdown")
packageVersion("knitr")
reticulate::repl_python()
