---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Haeram Han HH26735

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
library(MASS)
VA <- data.frame(VA)
# if your dataset needs tidying, do so here
VA$treat <- ifelse(VA$treat == 1,"standard","test")
head(VA)
# any other code here
?VA
VA %>% group_by(status) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(cell) %>% count()
VA %>% group_by(prior) %>% count()

```
This data was found using the github linked to this project. I found this dataset interesting because as someone who wants to go into the medical field, it caught my eye when searching. The data set is about Veteran's administration lung cancer trail. The data variables are stime, status, treat, age, Karn, diag.time, cell and prior. Stime is the number of days people survive after the trail or the follow up time. Status measures if the patient is dead or censored. treat means if the treatment they received was standard or test. Age is the patient's age in years. Karn is the karnofskysocre patient's performance on a scale of 0 to 100 this score measures how functional the patient is. Diag.time is the time since the diagnosis in months. Cell is one of the four cell types and prior is if the patient has had any prior therapy. There are 137 observations stime, age, Karn and diag.time. For status,9 were dead and 128 were censored. For treatment, 69 were on the standard and 68 were test. Cell type 1 had 35, Type 2 had 48, Type 3 and Type 4 had 27 observations. For prior variables 97 did have a prior treatment while 40 did not have a prior treatment.
### Cluster Analysis

```{R}
library(cluster)
# clustering code here
VA_dat <- VA %>% dplyr::select(stime,age, Karn)

sil_width<- vector()
for (i in 2:10){
 pam_fit <- pam(VA_dat, k=i)
 sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_point(aes(x=1:10,y=sil_width))+geom_path(aes(x=1:10,y=sil_width))+xlab("clusters")+scale_x_continuous(name="k", breaks=1:10)

VA_pam <- VA_dat %>% pam(k=2)
plot(VA_pam, which= 2)
VA_pam$medoids

VA %>% slice(VA_pam$id.med)
library(GGally)
VA <- VA %>% mutate(cluster=as.factor(VA_pam$clustering))
ggpairs(VA, cols= 1:6, aes(color=cluster))

```

Discussion of clustering here
The highest maximum average silhouette width was cluster two as seen in the the silhouette plot. The silhouette width average was 0.66 meaning it has a reasonable cluster solution. Between the two clusters, it seems like each cluster has the same Karn value meaning that the patients in these clusters have similar functional values. Usually the higher the Karn value, the better so these patients have a mediocre average value.
In the ggpair plot there are only a few significant positive correlation which are prior and diag.time and Karn and stime. This means that if there is a prior treatment the higher diagnosis time there is. Also, stime and Karn means that the more survival time a patient has the higher Karn value they have. This may make sense because the more you live from the treatment the healthier life you'll most likely live. Stime and Karn had a correlation value of 0.362. Prior and diag.time had a correlation value of 0.418 The highest two negative correlations were age and Karn and age and stime. Age and Karn has a value of -0.096 which also makes sense because the older one gets, the less functional value one has. Age and stime have a value of -0.068 meaning that the older one gets the less survival time. 
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
princomp(VA_dat, cor=T) -> pca1
VA_dat
summary(pca1)
pcavar <- pca1$sdev^2

varprop = round(pcavar/sum(pcavar), 2)

ggplot() + geom_bar(aes(y = varprop, x = 1:3), stat = "identity") + 
    xlab("") + geom_path(aes(y = varprop, x = 1:3)) + 
    geom_text(aes(x = 1:3, y = varprop, label = round(varprop, 
        2)), vjust = 1, col = "white", size = 5) + 
    scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) + 
    scale_x_continuous(breaks = 1:10)
                       
round(cumsum(pcavar)/sum(pcavar), 2)
round(cumsum(pcavar)/sum(pcavar), 3)
summary(pca1, loadings = T)

pca1mat <- pca1$scores %>% cor %>% round(10)

library(dplyr)

VA_dat %>% as.data.frame %>% dplyr::select(age) %>% mutate(PC1=pca1$scores[,1],
                                                         PC2= pca1$scores[,2])%>%
  ggplot(aes(PC1,PC2, color = age)) +geom_point()

```

Discussions of PCA here. 
There are only 3 PCs which explain 85% of the total variance. PC1 has a high magnitude and sign for stime and Karn meaning that these individuals have a higher survival time and a higher functional value/Karn value. The negative for age indicates these individuals are on the lower end of the age spectrum. PC2 who that all the signs are positive meaning that individuals in this will have a higher stime, higher age and higher Karn but, these individuals do not have as high stime and Karn as PC1. PC3 individuals have a higher stime but a lower Karn functional value. 
The ggplot shows that as PC1 increases age does not really affect PC1. However, as PC2 increases so does the age. This shows there is a correlation between PC2 scores and age. 
###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(treat== "standard" ~ stime + age + Karn + diag.time, data=VA, family="binomial")
logistic_fit
prob_reg <- predict(logistic_fit, type="response")
class_diag(prob_reg, truth=VA$treat, positive ="standard")

y <- VA$treat
y<- factor(y, levels=c("standard","test"))
y_hat <- sample(c("standard","test"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("standard","test"))
table(actual = y, predicted = y_hat) %>% addmargins()
```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k=10

data<-sample_frac(VA) 
folds <- rep(1:k, length.out=nrow(data)) 

diags<-NULL

i=1
for(i in 1:k){
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$treat

as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}

summarize_all(diags,mean) 
```

Discussion here
The logistic fit had an AUC of 0.55 meaning that this model is poor. So this means that treatment is a poor in distinguishing between the classes of stime, age, Karn and diag.time. However after doing the cross validation the AUC decreased to 0.54673 meaning that the model is still poor and that what treatment the patient received is not a good distinguishing factor between the classes. Since the model has a small gap between the AUC this means that there isn't much overfitting but rather underfitting.
For the confusion matrix, 0.4928 or 49.28% of the patients actually had a standard treatment while 0.5072 or 50.72% didn't have the standard treatment. The true positive rate(TPR) is 0.4928 and the the false negative rate(FNR) is 0.5072 so this model is more likely to give a false negative for standard. For test 0.5441 is the TPR and the FNR is 0.4559 meaning that this model is more likely classify patients that did the test treatment better than the standard treatment. 
### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)

#your code here
prob_knn <- predict(knn_fit,newdata = VA)

class_diag(prob_knn[,2], truth=VA$treat, positive="standard")

knn_fit2 <-knn3(factor(treat=="standard",levels=c("TRUE","FALSE")) ~ stime + age + Karn + diag.time, data=VA, k=5)
y_hat_knn <- predict(knn_fit2,VA)
table(truth= factor(VA$treat== "standard", levels=c("TRUE","FALSE")),
      prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k=10

data<-sample_frac(VA) 
folds <- rep(1:k, length.out=nrow(data)) 

diags<-NULL

i=1
for(i in 1:k){
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$treat
fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA) 
probs <- predict(fit, newdata=test)[,2]

diags<-rbind(diags,class_diag(probs,truth, positive="standard")) 
}

summarize_all(diags,mean)
```

Discussion
For the non parametric classifier the AUC without cross validation shows has a value of 0.728 meaning this model is pretty fair. However, after cross validation the AUC had a value of 0.705. This gap is somewhat small meaning there is some underfitting happening. Overall this model is better at predicting new observations than the linear classification. This model had an AUC value of 0.70 meaning that this model is fair. 
### Regression/Numeric Prediction

```{R}
# regression model code here
fit<-lm(age~.,data=VA)
yhat_reg <-predict(fit)
yhat_reg
mean((VA$age-yhat_reg)^2)
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 
data<-VA[sample(nrow(VA)),]
folds<-cut(seq(1:nrow(VA)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(age~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$age-yhat)^2) 
}
mean(diags)

```

Discussion
The mean MSE or Mean squared error for the overall data is 105.0733 however the average MSE for the k testing fold is 109.1678. The MSE is really high meaning there is a high prediction error involved for this data set. The CV is higher than the MSE meaning that there is underfitting which is consistent with the previous cross validation.  
### Python 

```{R}
library(reticulate)
use_python("/usr/bin/python3", required = F)

VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC


```

```{python}
# python code here
r.mean_KC

import numpy as np
VA= r.VA
np.mean(VA["age"])
Age_mean = np.mean(VA["age"])

```


```{R}
py$Age_mean
```

Discussion
This part was to use the library reticulate to communicate between python and R. In the R chunk, I created the variable mean_KC which is the mean KARN score based on cell type and in the python chunk I was able to show this data in the python chunk using r.mean_KC. In the Python chunk I had to import numpy library to get the mean of age for the VA dataset. Using the np.mean function in python I was able to create a variable and use this variable in the R code chunk by py$Age_mean.
### Concluding Remarks

Include concluding remarks here, if any
I absolutely loved this class, although it was super frustrating when the code was not working at the end, when everything works, it is EXTREMELY satisfying.
