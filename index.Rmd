---
title: 'Project 2: Data Mining, Classification, Prediction'
author: "SDS322E"
date: ''
output:
  html_document:
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: yes
  pdf_document:
    toc: no
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", warning = F, message = F,
tidy=TRUE, tidy.opts=list(width.cutoff=60), R.options=list(max.print=100))

class_diag <- function(score, truth, positive, cutoff=.5){

  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))

  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[1,1]/rowSums(tab)[1]
  spec=tab[2,2]/rowSums(tab)[2]
  ppv=tab[1,1]/colSums(tab)[1]

#CALCULATE F1
  f1=2*(sens*ppv)/(sens+ppv)
  
#CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc,sens,spec,ppv,f1,ba=(sens+spec)/2,auc, row.names = "Metrics"),4)
}
```

# Mining, Classification, Prediction

## Haeram Han HH26735

### Introduction 

Paragraph or two introducing your datasets and variables, why they are interesting to you, etc. See instructions for more information

```{R}
library(tidyverse)
# read your datasets in here, e.g., with read_csv()
library(MASS)
VA <- data.frame(VA)
# if your dataset needs tidying, do so here
VA$treat <- ifelse(VA$treat == 1,"standard","test")
head(VA)
# any other code here
?VA
VA %>% group_by(status) %>% count()
VA %>% group_by(treat) %>% count()
VA %>% group_by(cell) %>% count()
VA %>% group_by(prior) %>% count()

```
This data was found using the github linked to this project. I found this dataset interesting because as someone who wants to go into the medical field, it caught my eye when searching. The data set is about Veteran's administration lung cancer trail. The data variables are stime, status, treat, age, Karn, diag.time, cell and prior. Stime is the number of days people survive after the trail or the follow up time. Status measures if the patient is dead or censored. treat means if the treatment they received was standard or test. Age is the patient's age in years. Karn is the karnofskysocre patient's performance on a scale of 0 to 100 this score measures how functional the patient is. Diag.time is the time since the diagnosis in months. Cell is one of the four cell types and prior is if the patient has had any prior therapy. There are 137 observations stime, age, Karn and diag.time. For status,9 were dead and 128 were censored. For treatment, 69 were on the standard and 68 were test. Cell type 1 had 35, Type 2 had 48, Type 3 and Type 4 had 27 observations. For prior variables 97 did have a prior treatment while 40 did not have a prior treatment.
### Cluster Analysis

```{R}
library(cluster)
# clustering code here
VA_dat <- VA %>% dplyr::select(stime,age, Karn)

sil_width<- vector()
for (i in 2:10){
 pam_fit <- pam(VA_dat, k=i)
 sil_width[i] <- pam_fit$silinfo$avg.width
}
ggplot()+geom_point(aes(x=1:10,y=sil_width))+geom_path(aes(x=1:10,y=sil_width))+xlab("clusters")+scale_x_continuous(name="k", breaks=1:10)

VA_pam <- VA_dat %>% pam(k=2)
plot(VA_pam, which= 2)
VA %>% slice(VA_pam$id.med)
library(GGally)
VA <- VA %>% mutate(cluster=as.factor(VA_pam$clustering))
ggpairs(VA, cols= 1:6, aes(color=cluster))

```

Discussion of clustering here
    
    
### Dimensionality Reduction with PCA

```{R}
# PCA code here
princomp(VA_dat, cor=T) -> pca1
VA_dat
summary(pca1)
pcavar <- pca1$sdev^2

varprop = round(pcavar/sum(pcavar), 2)

ggplot() + geom_bar(aes(y = varprop, x = 1:3), stat = "identity") + 
    xlab("") + geom_path(aes(y = varprop, x = 1:3)) + 
    geom_text(aes(x = 1:3, y = varprop, label = round(varprop, 
        2)), vjust = 1, col = "white", size = 5) + 
    scale_y_continuous(breaks = seq(0, 0.6, 0.2), labels = scales::percent) + 
    scale_x_continuous(breaks = 1:10)
                       
round(cumsum(pcavar)/sum(pcavar), 2)
round(cumsum(pcavar)/sum(pcavar), 3)
summary(pca1, loadings = T)

pca1mat <- pca1$scores %>% cor %>% round(10)

library(dplyr)

VA_dat %>% as.data.frame %>% dplyr::select(age) %>% mutate(PC1=pca1$scores[,1],
                                                         PC2= pca1$scores[,2])%>%
  ggplot(aes(PC1,PC2, color = age)) +geom_point()

```

Discussions of PCA here. 

###  Linear Classifier

```{R}
# linear classifier code here
logistic_fit <- glm(treat== "standard" ~ stime + age + Karn + diag.time, data=VA, family="binomial")
logistic_fit
prob_reg <- predict(logistic_fit, type="response")
class_diag(prob_reg, truth=VA$treat, positive ="standard")

y <- VA$treat
y<- factor(y, levels=c("standard","test"))
y_hat <- sample(c("standard","test"), size=length(y), replace=T)
y_hat <- factor(y_hat, levels=c("standard","test"))
table(actual = y, predicted = y_hat) %>% addmargins()
```

```{R}
# cross-validation of linear classifier here
set.seed(322)
k=10

data<-sample_frac(VA) 
folds <- rep(1:k, length.out=nrow(data)) 

diags<-NULL

i=1
for(i in 1:k){
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$treat

as.factor(VA$treat)
fit <- glm(as.factor(treat) ~ stime + age + Karn + diag.time, data=train, family= "binomial")
probs <- predict(fit, newdata=test, type="response")
diags<-rbind(diags,class_diag(probs,truth, positive="standard"))
}

summarize_all(diags,mean) 
```

Discussion here

### Non-Parametric Classifier

```{R}
library(caret)
# non-parametric classifier code here
knn_fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA)

#your code here
prob_knn <- predict(knn_fit,newdata = VA)

class_diag(prob_knn[,2], truth=VA$treat, positive="standard")

knn_fit2 <-knn3(factor(treat=="standard",levels=c("TRUE","FALSE")) ~ stime + age + Karn + diag.time, data=VA, k=5)
y_hat_knn <- predict(knn_fit2,VA)
table(truth= factor(VA$treat== "standard", levels=c("TRUE","FALSE")),
      prediction= factor(y_hat_knn[,1]>.5, levels=c("TRUE","FALSE")))
```

```{R}
# cross-validation of np classifier here
set.seed(322)
k=10

data<-sample_frac(VA) 
folds <- rep(1:k, length.out=nrow(data)) 

diags<-NULL

i=1
for(i in 1:k){
train<-data[folds!=i,] 
test<-data[folds==i,] 
truth<-test$treat
fit <- knn3(treat=="standard" ~ stime + age + Karn + diag.time, data=VA) 
probs <- predict(fit, newdata=test)[,2]

diags<-rbind(diags,class_diag(probs,truth, positive="standard")) 
}

summarize_all(diags,mean)
```

Discussion


### Regression/Numeric Prediction

```{R}
# regression model code here
fit<-lm(age~.,data=VA)
yhat_reg <-predict(fit)
yhat_reg
mean((VA$age-yhat_reg)^2)
```

```{R}
# cross-validation of regression model here
set.seed(1234)
k=5 
data<-VA[sample(nrow(VA)),]
folds<-cut(seq(1:nrow(VA)),breaks=k,labels=F) 
diags<-NULL
for(i in 1:k){
  train<-data[folds!=i,]
  test<-data[folds==i,]
  fit<-lm(age~.,data=train)
  yhat<-predict(fit,newdata=test)
  diags<-mean((test$age-yhat)^2) 
}
mean(diags)

```

Discussion

### Python 

```{R}
library(reticulate)
library(tidyverse)
use_python("/usr/bin/python3", required = F)

VA %>% group_by(cell) %>% summarize(mean(Karn)) -> mean_KC
```

```{python}
# python code here

```

Discussion

### Concluding Remarks

Include concluding remarks here, if any




